* DailyWork.org 
** this org file is used to note *daily work*
*** make sure the meaning of index/search analyzer
:LOGBOOK:
CLOCK: [2015-08-25 周二 14:52]--[2015-09-02 周三 17:13] => 194:21
** the form of the file is to be determined
- now form ::
  - first :: note as data in heading
  - two :: in the heading just below : conclude this day's life
* ! CAUTION :
** the *id is the main-key, so must be indexed
** [#A] the way to become the god : solve the problem again and again
*** if had time, search origin code to solve issue
* ! PRINCIPLE :
** first finish work, then the other.
** first write a prototype which can run and implement basic functions , then iterates to develop
- basic :: is the most important
** state TODO to DONE:
- remember to write the reason ::
* WORK ACCUMUALTE:
** scrapy to find *data scratching* work
** deep in elasticsearch to find *search* work
** JS to find *front-end* work
* WORK CAUTION:
** be cautious to solid basic knowledge about CS
** accumulate work experience
* <2015-08-26 周三 > [5/5]
** DONE send the documents from kedaxunfei to haizi
CLOSED: [2015-08-26 周三 14:00]
大体上有个数了,个人觉得应该是不靠谱，给 haizi 看看再说吧
:LOGBOOK:
CLOCK: [2015-08-26 周三 13:42]--[2015-08-26 周三 14:00] =>  0:18
:END:
** DONE communicate in QQ with turing rebot 
CLOSED: [2015-08-28 周五 09:58]
conclusion : not reliable
** DONE reorgnize query DSL in ES, then to send to haizi
CLOSED: [2015-08-28 周五 09:58]
*** Java API : use_dis_max 
** DONE receive email from xiaoi
CLOSED: [2015-08-28 周五 09:58]
** DONE travelnote type "9999+" how to define the mapping
CLOSED: [2015-09-21 周一 11:01]
- manually change from "9999+" to "9999" or "10000",
* <2015-08-27 周四 10:19>[6/6]
** DONE use Python to generate one csv file and upload the file to xfyun
CLOSED: [2015-09-02 周三 15:28]
*** the length limit of question and answer
*** DONE csv cquestion haractor code 
CLOSED: [2015-08-27 周四 18:08]
xfyun needs the GBK code 
**** DONE need *code snippet*
CLOSED: [2015-09-02 周三 15:28]
~import csv

# with as
with open('csv-name','wb') as csvfile:
    csvwriter = csv.writer(csvfile)
    a = ..
    b = ..
    csvwriter.writerow((a,b)
~
**** *python* encode/decode chinese characters tool          :python code:
- *must* the process : unicode to encode in gb18030
- *decode* ('') : decode the chinese characters as the ('') to *unicode*
- *encode* ('') : encode the chinese characters as the ('')
- *gb18030* : all unicode, *attention* : is no gb *k* 18030
  - it is wrong :: this question belong to environment
- finally :: code question is easy to be confusing,
  - solution :: good habit and careful
** DONE use Python to generate one xls about the code question
CLOSED: [2015-09-02 周三 15:28]
xlwt.Workbook(encoding='gb18030')
sheet.write the contents needed to be decoded and encoded in 'gb18030'
make sure the default code of the [] 
*** DONE should need *code snippet* 
CLOSED: [2015-09-02 周三 15:21]
~import xlwt

file = xlwt.Workbook(encoding='18030')
# set cell_overwrite_ok to ok default is false
sheet = file.add_sheet('my_qa', cell_overwrite_ok=True)
sheet.write(row, col, value)
file.save('name.xls')
~
*** es the "accepted" is not indexed ,so only to the mongoDB
query the answer type and accepted ture to find the qid
use the qid to find the question
Q&A: question : title
     answer : contents
** DONE python syntax: with context. refer the python basis tutorial
CLOSED: [2015-09-02 周三 16:20]
simply to say : the with...as.. syntax is 
 the live period in the code block of one object
 for instance : it is automatically to close the file object

** DONE offer the es client java and curl: only the case than given the question 
CLOSED: [2015-08-27 周四 14:18]
** DONE email from xiao i
CLOSED: [2015-08-27 周四 14:18]
** DONE test xiaoi and xfyun  
CLOSED: [2015-08-28 周五 18:50]
* <2015-08-28 周五 09:58>[2/2]
** another intelligent serviice : http://www.faqrobot.org/
** DONE send the resume to wangbingqing at noon <2015-08-28 周五 12:10>
CLOSED: [2015-08-28 周五 16:26] SCHEDULED: <2015-08-28 周五 12:10>
** pass value in python :
in python: the basic type is passed by value
           the complex type is passed by pointer
CLOSED: [2015-09-21 周一 11:06]
* <2015-08-31 周一 10:21>[3/4]
** DONE 1. programe python to delete data in xunfy, choice : request or scrapy.request
CLOSED: [2015-08-31 周一 17:13]
CLOCK: [2015-08-31 周一 11:04]--[2015-08-31 周一 11:29] =>  0:25
CLOCK: [2015-08-31 周一 10:26]--[2015-08-31 周一 10:51] =>  0:25
<2015-08-31 周一 10:26>
*** first write the code and then debug the burpsuite
*** DONE learn how to use burpsuite to intercept 
CLOSED: [2015-08-31 周一 14:57]
CLOCK: [2015-08-31 周一 14:23]--[2015-08-31 周一 14:48] =>  0:25
**** first : open the proxy configuration 
**** second : configure burpsuite Proxy->Options->Proxy Listeners
**** third : configure the brower's proxy is the burpsuite Proxy Listeners
**** do not configure intercept is on if don't want to intercept the http package
** only a timestamp : <2015-08-31 周一 18:19>
** DONE make the python file to run in server
CLOSED: [2015-09-01 周二 17:48]
*** pycharm deploy remote code steps:
1. copy a project to a local directory
2. configure : tools->deployment set right configuration
3. make deployment automatic: tools configuratio
** DONE synchronize the data between the mongoDB  saved by running spider and the program runing needed
CLOSED: [2015-09-21 周一 11:18]
:end: logstash / mongoDB connector
** TODO make it clear that the ssh-key   
* <2015-09-01 周二 10:34> [1/3]
** TODO First done the virtualenv, the study docker with python 
*** Python : virtualenv                                       :usualy used:
- operation : pip install virtualenv
               mkdir <own-dir>
               virtualenv --distribute <own-dir>  
               (--distribute can automatically install pip in the <own-dir>)
- activate : cd <own-dir>
              source bin/activate
- deactivate: when in the virtuanenv dir, type: deactivate
- use cmd which python to make it clear 

** DONE find the question the mongo not vaild cursor id 
CLOSED: [2015-09-01 周二 17:45]
- pymongo 3.0 : coll.find(no_cursor_timeout=False)
- remember cursor.close()
** TODO conclude the query DSL in mindmap
* <2015-09-02 周三 14:18>[1/1]
** DONE the reindex mechanism in es
CLOSED: [2015-09-21 周一 11:10]
1. create a new index 
2. pull data from old index and then push data to new index use bulk API
3. about alias question :
   use alias to avoid change the application's code when reindexing
4. the precondition of alias
   the application visit the alias which points to the old indexed
5. need "_source"
*** conclusion : use the alias in application's code insted of indices
*** the method to create one alias for one index :
PUT /my_index_v1  // 
PUT /my_index_v1  //
pu  t

*** need the code snippet
PUT 
*** python elasticsearch is easy 
- elasticsearch.helpers.reindex() is ok;
* <2015-09-03 周四 09:19>[2/2] 
** DONE write email to Teacher Cheng
CLOSED: [2015-09-03 周四 09:57]
:LOGBOOK:
CLOCK: [2015-09-03 周四 09:28]--[2015-09-03 周四 09:53] =>  0:25
:END:
<2015-09-03 周四 09:27> 
** DONE depolyment the layout of git-org in github
CLOSED: [2015-09-21 周一 11:12]
*** draw a tree of the layout, include : the naming advantage and reason
* <2015-09-06 周日 12:23>[2/2]
** DONE practise the reindex mechanism
CLOSED: [2015-09-21 周一 11:09]
:LOGBOOK:
CLOCK: [2015-09-06 周日 15:55]--[2015-09-06 周日 16:49] =>  0:54
CLOCK: [2015-09-06 周日 15:25]--[2015-09-06 周日 15:50] =>  0:25
:END:
not completed reason : one part is the unknown network problem
** DONE use xunfei SDK in android
CLOSED: [2015-09-21 周一 11:08]
- reason : useless, and find java platform api
* <2015-09-07 周一 10:06>[2/9] 
** TODO the principle of agile software development
** TODO ask the question in stackoverflow
leave a unresolved and strang question
** TODO attachment type
** TODO search all the types include one index 
solutions : count the type number via mapping 
seem like no perfect method
** DONE search the '9999+' type mapping 
CLOSED: [2015-09-21 周一 11:14]
:end: manually dealed with it 
** TODO look the site : http://stackoverflow.com/questions/14465668/elastic-search-multiple-indexes-vs-one-index-and-types-for-different-data-sets
** DONE learn to add alias for index
CLOSED: [2015-09-07 周一 20:09]
*** _alias & _aliases
*** check alias api : GET /my_index_v1/_alias/*
***                 : GET /*/_alias/my_index
*** create alias api : PUT /my_index_v1,  PUT /my_index_v1/_alias/my_index
*** create alias api : add more add operations when need many aliases
POST /_aliases 
{
 "actions" : [   {"remove" : { "index" : "my_index_v1", "alias" : "my_index"}},
                 {"add" :    { "index" : "my_index_v2", "alias" : "my_index"}}
             ]
}
* <2015-09-08 周二 11:18>[1/2] 
** DONE ask if it has restapi and if not, what is the way in java
CLOSED: [2015-09-21 周一 11:16]
:end: rubbish: the co-called technology support 
*** business custom has the restapi, the follow-ups are consulting
** the first and second calss statistic error 
*** the story of 'qurt' series constant int 
*** how to use the result returned from java
** TextUnderstander : 文本转语义
** TODO callback mode
* <2015-09-10 周四 10:41>
** abnf, use dict
#include "dictname.lst"
...  $city = $u_LST_dictname
** docker copy file from container to local
- docker cp containerid:/original/file-path /target/file-path
** scp ssh-key login
** pymongo longin auth
* <2015-09-11 周五 10:41>[1/5] 
** judge character number in a string:
use len() under unicode
** write file in python:
1) with open('filename', 'w') as f:
     f.write(content + '\n')     
2) the most simple operation is useful, while can not use package certainly
** making a functions is useless if it depends on the context
function is seperated to reuse
** DONE learn the abnf and make notes well 
CLOSED: [2015-09-21 周一 10:47]
:LOGBOOK:
CLOCK: [2015-09-11 周五 13:43]--[2015-09-11 周五 14:10] =>  0:27
:END:
*** ABNF FILE syntax
**** 写一个文件，例：北京有什么好玩的地方
#ABNF 1.0 UTF-8
#includ 'dictname.lst"
// output the $name rule
root name;
#ABNF HEAD-END
// #ABNF can end with ';' 

// two forms below are both effective
$want = 有{location:have};
$city {location:city} = 北京 | 上海;
// quote the include dict
$city = $u_LST_dictname
$adj = 好玩的;
// biz:play returns the 'service' segment
$main {biz:play} = $city [$want] 什么 [$adj] [地方]

// 需求1: 整个问题的架构是什么?
// 需求2: 希望抽取出什么信息
***** the result abnf returns
"service" : if biz appeared
"text"    : the input string
"semantic": the returned semantics
"rc"      : mark code, 0 is normal
**** annotation is the same as cpp
*** header segment: had better end with ";"
#ABNF 1.0 UTF-8;
root vairety_name;
#ABNF HEAD-END;

dict name later to learn, because not cncouter yet
*** dict name can only allow int,char,underline
*** grammar form : $vName = vDefinition   this form is called 'rule' :abnf:
**** vName declaration : 
1. begin with '$', end with white. 
2. Chinese is not recommanded
**** vDefinition : 
**** variable reference :
three varieties : local/external/variable reference
focus on local reference
**** support wild character, (later make it clear)
1. '$_ti_ch_' :: wild character
**** operator:
1. '$' '@' : both can be used for declaration, while '$' is only in reference
2. '{}' : semantic symbol,can not be nested
2.1     : content in {}, will be the nodes in returned parseed xml
3. '|' : choose
4. '[]' : choose or none
5. '()' : group
6. '<int>' : repeat int times.  <int+> <int-int>
7. ? :: can add to semantic content, later to research.now, only return semantic
**** $query {operation%query} = check :
1. add a new node 'operation' ,the level is equal to 'semantic' node
2. its value is the $query
3. can not add {operation.query}, in this way ,it is the node in 'sematntic'
**** CONCLUSION : the trick to write xunfei abnf
- first :: make sure the core element that will be matched
  + can be very very short by cutted down the unnecessary part
- second :: the places the can be ambigous and useless can be used wild characters
- watch out the use of combination of wild characters and '<>'
- so, the left work is only to make sure the core phrase mathing some scene
** TODO abnf is very simliar to regex, so review the regex
** TODO logstash/mongoDB connector haizi  
** TODO callback rate standard books?
* <2015-09-15 周二 10:32>[4/4] 
** DONE write the relevant ABNF based to the scenes from haizi
CLOSED: [2015-09-21 周一 10:43]
:LOGBOOK:
CLOCK: [2015-09-15 周二 10:49]--[2015-09-15 周二 11:14] =>  0:25
:END:
** DONE xunfei semantics sequence                        :xunfei semantics:
CLOSED: [2015-09-21 周一 10:42]
私有语义 --> 私有问答 --> 通用问答--> 通用语义
** DONE make sure the data in need
CLOSED: [2015-09-21 周一 10:42]
- the qa needed in xunfei from mongoDB
- the qa needed in es
** DONE the form is my decision?                               :work issue:
CLOSED: [2015-09-21 周一 11:13]
:end: haizi decides
- should return what segment?
- need what information
- the information is used to decide to which interface
* <2015-09-16 周三 13:21>[2/2] 
** DONE communicate with topy about the interface to make rules of how to answer the message
CLOSED: [2015-09-21 周一 10:41]
- [X] make sure the individual scene one by one
  - [X] viewspot :: 
  - [X] restaurant ::
  - [X] travelnote :: 
- look at the apireturn.org file 
* <2015-09-17 周四 13:51>[5/6] 
** TODO learn how to operate jira
** DONE first write the python program to process successfully
CLOSED: [2015-09-21 周一 10:40]
** DONE then connect it with ApiumWorker
CLOSED: [2015-09-21 周一 10:40]
** about thrift :
*** a RPC(Remote Procedure Call) protocal cross language 
*** need write .thrift file and realize the function by self
*** then use tools of different languages to generate responding code
** learn git how to change branch
** json.dumps() : 
- comprehension : used to convert json format to string
- requests.get() only accept string
** DONE write a python script to upload the Q&A with accepted in es?
CLOSED: [2015-09-21 周一 10:40]
** DONE consider the operation : get data from es, after processing, then upload to es 
CLOSED: [2015-09-21 周一 10:40]
- the q&a should be processed to ensure one question have an answer at least
** DONE the min_score in es query :
CLOSED: [2015-09-21 周一 10:40]
{
  "min_score" : 2.9,
  "query": {
     "term" : { "user" : "kimchy"}
  }
}
** write code : write down the thought and annotation in time and everywhere
- especially : when tired, it is very easy to puzzle about what you do.
- more worsely, it is terrible when puzzle about the thought ever have
** python code : 
- keep writing down chinese characters with 'u'
* <2015-09-18 周五 10:09>[2/3] 
** TODO python code question about message[2/3]
*** DONE how to deal with message module in *.py
CLOSED: [2015-09-21 周一 10:39]
- import is ok
*** DONE upload the accepted q&a to es and xunfei?
CLOSED: [2015-09-21 周一 10:10]
- [X] to xunfei
- [X] to es ? 
*** TODO finish the left part about message[0/2]
- [ ] the es query sentence
  - [ ] ues multimatch
  - [ ] about weight and score
- [ ] restaurant and travelnote
*** **kwargs :: means the para form of 'a=b, c=d'
*** when ecounter *list*, watch out the index error, whether list is null
- python robust :: watch out list
*** pycharm operation : 
- debug as step :: F8
*** question: 
- restaurant :: a lot of places does not open:?
- viewspot :: do not support country, only city?
- travelnote :: returned summary is too long
** work time system:
- work and relax are necessary
- pomodora ? 
** program princile :
- consider more robust
- first program one edition which can work
- then iterate it, ex : divide into different modules, etc.
** DONE first return one text sentence, then a card,
CLOSED: [2015-09-21 周一 10:38]
- a card :: means the message client can recognize
** DONE test sentence:[5/5]
CLOSED: [2015-09-21 周一 10:39]
- [X] intelligent q&a :: 南京11月天气怎么样阿
- [X] viewspot :: 
  - [X] 正向 :: 北京/南京有什么景点
  - [X] 反向 :: 中国有什么景点， 应返回'请输入城市名称'
- [X] restaurant :: 南京有什么好吃的
- [X] travelnote :: 厦门的游记
- [X] es :: 新西兰的签证
* <2015-09-21 周一 10:24>[0/6] 
** TODO conclude elasticsearch basic operation
- to file [[elasticsearch.org]]
# elasticsearch.org
DEADLINE: <2015-09-25 周五>
- main work :: upload to es, and mapping,reindex,etc. basic operation
- basic ::
  - create index ::
  - mapping ::
  - alias ::
  - delete :: 
  - Python elasticsearch ::
    - helpers.reindex ::
    - helpers.bulk ::
    - :: 
** TODO conclude mongoDB basic operation
** TODO review all the code writed in Lxp, conclude and document
** TODO conclude git basic operation
** TODO scrapy basic frame
** TODO wenwen Questions:
- api :: locality support?
- es :: use alias in code
  - some details :: min_score, q&a,

* <2015-09-22 周二 10:18>[0/2] 
** TODO today's work : about elasticsearch
- note and conclude
** elasticsearch basic operation :
*** PUT & POST
- PUT :: explicitly know the url location to create or change
- POST :: call a operation ? 
*** TODO note mapping operation:
- mapping :: 
  - what is mapping ::
    - :: 
  - dynamic mapping is "false" means ::  
- default mapping :: PUT /index { "mapping": {"_default_":{}}
- upload ::
  - PUT /index  { "mapping" : { "doc_type" : { "_all": {"enabled": "false" } ,"dynamic":{...}, "proterties" : { "field" : { "type" : ..., "analyzer" : "..", "index" : "..", "term_vector": "with_positions_offsets" }}}}}
    - update a new text field ::
      + PUT /index/_mapping/doc_type { "properties" : {"new-field": {} .} 
  - POST /index/doc_type/_mapping  { "doc_type" : ...}
  - "store" ::
    - default is false because "_source" default is true
  - conclude :: 
    - make sure the http method :: PUT or POST
    - proterties :: type,analyzer,index,term_vertor(used for highlight)
      - core type :: integer,long,string,nested
        - string 
        - byte,short,integer,long
        - float,double
        - boolean
        - date
- _all / custom analyzer / 
** TODO make mapping : one by one [0/3]
*** TODO mapping :: 
  - "dynamic" :: "strict",
    - can dynamic strict and some field to dynamic true::
      - "properties" : {"title" : {"type" : "object", "dynamic" : true}}
    - 
  - search field :: "type":"string", "analyzer": "ik", "term_vector": "with_positions_offsets"
  - "*id" :: as the main key , must be indexed
*** TODO upload es
:LOGBOOK:
CLOCK: [2015-09-22 周二 15:37]--[2015-09-22 周二 16:02] =>  0:25
:END:
*** TODO query sentence :: 
  - search field :: "query":{ "multi_match", "match"}
  - return field ::
    - fielddata_fields ::
      - use case:: not all, {"query" : {..}, "fielddata_fields" : ["field_name": ["field1","field2",...]}
      - results :: hits will not change and add a "fields" only contains "field_name" which indexed
    - fields ::
      - use case :: {"query": {}, "fields": ["field1"..]}
      - results :: do not have "hits", and contains the "fields" unless index or not
  - rank field :: to use this field to score, : function field.value
    - not sure whether need to set mapping index ::
      - need to index the field used to field_value_factor:: 
    - use es to text :: 
  - score :: boost
** mongoDB 
- [[http://www.cnblogs.com/xffy1028/archive/2011/12/03/2272837.html][Mongo db 与mysql 语法比较]] : 借这个机会学习sql
*** db.collections.find({"a": {$exits: true}})
* <2015-09-23 周三 10:38>[/] 
** elasticsearch search nested object
** query default & or 
** "store" change to enable
- think about all types to "store" "index"
  - think "dynamic_template" ??
* <2015-09-24 周四 10:33>[0/1] 
** TODO set all fields store and index: mapping
:LOGBOOK:
CLOCK: [2015-09-24 周四 10:33]--[2015-09-24 周四 10:58] =>  0:25
:END:
- dynamic_templates :: "mappings" in one type
  - PUT /my_index
    {
        "mappings": {
            "type_name": {
                "dynamic_templates": [
                    { "store_generic": {
                          "match": "*",
                          "mapping": {
                              "store": true,
                              "index": "no"
                          }
                    }}
                ]
    }}}
- _default_ :: use in different types in one index 
** db.TravelNote all to es
- dynamic is dynamic_templates
- pointed segament is pointed
  - "store" of all fields is ture and "index" of no search and sort is no
- store ture is used to return search result
** elasticsearch
- index name : must lowercase
- del mapping ?
** mongoDB
- sort by _id , xuchuan can from the last place to start
  - mongoDB sort default is $natural order
    - pymongo :: coll.find().sort("_id", 1)  1 : ascending, -1 : descending
    - mongoDB :: db.coll.find().sort({"_id" : 1})
    - attention :: db.coll.find().sort({"_id" : 1}).skip(100).limit(10)
      - limit :: the result num returned
      - skip :: begins from all the sorted data, skip 100, then return limit num
** scrapy 
** docker
* <2015-09-25 周五 09:56>[/] 
** how to ensure the data in es 
** test query in native client
** elasticsearch query and function score query
- a small example
  - {
      "query" : {
        "function_score" : {
          "query" : {...},
          "functions": [
            { "weight" : ..,     "weight" is the function boost
              "field_value_factor" :{
                "field" : "field_name",
                "factor" : 1.2,
                "modifier" : "sqrt",
                "missing" : 1,    // the value will be use if the field does not exist
               // sqrt(1.2*field_name.value)
              }}
          ],
          "score_mode" : "..",     // the way to deal with functions' score if there are many functions 
          "boost_mode" : "..",     //the way processing query score and function score
        },
      }
    }
- attention ::
  - "query" and "functions" are in "function_score"
  - "function_score"
** Java
- Java basic type
  - 1.2f :: with "f" ,it is the float type
* <2015-09-28 周一 12:15> [/]  
** service in one host can be rebooted, but service for users must not be down
** mongo connector
* <2015-09-29 周二 10:23> [/]  
** mongoDB connector:
*** make clear the docmanager 
- func of different part
*** test Mongo Connector [1/1] 
- [X] run mongo connector first
  - [X] replica set
    - [X] first simple and crude run it testly in native
  - [X] test mongo connector with elasticsearch with a mapping
    - [X] new a test mongo in native, to es test index
    - [X] mongo connector will create new index in es which mongoDB have
    - [X] test : the index already exists, start mongo-connector
      - work correctly 
    - [X] test : the index already exists, but mapping is different
      - es can run correctly, but what does "dynamic is false" mean?
**** by the way, familiar to mongoDB basic operation : CURD
- CURD ::
  - C :: crate new db/collections/document
  - U :: update
  - R :: read
  - D :: delete
- mongod ::
  - mongod --dbpath ... ::
- mongo connector ::
  - mongod :: mongod --replSet setname
  - mongo :: mongo
    - rs.initiate()
  - cmd :: mongo-connector -m host:port -t host:prot -d elastic_doc_manager
  - special db ::
    - mongo-connector will no procee special dbs :: test,local 
** maven: 
- keep hello maven to test obj. quickly
- familiar to maven
* <2015-09-30 周三 12:35> [/] 
** mongo connector wiki
:LOGBOOK:
CLOCK: [2015-09-30 周三 14:14]--[2015-09-30 周三 14:39] =>  0:25
CLOCK: [2015-09-30 周三 13:42]--[2015-09-30 周三 14:07] =>  0:25
:END:
- oplog process file
  - keep track of the lastest oplog 
- resync
** optimize work after holiday
* <2015-10-08 周四 10:08> [/] 
** make sure the elasticsearch query: [0/3]
elasticsearch.org
- [ ] need :
  - [ ] search keyword in responding segment
  - [ ] need details
    - boost ::

- [ ] curl form
- [ ] java form
* <2015-10-09 周五 10:46> [/] 
** review technology depositing   
- elasticsearch
- mongoDB
** MVC (interview & career)  
** 2 abnf, 1, mongoDB connector
*** TODO [#A] mongoDB connector
*** TODO [#B] ABNF self-implement** ik, replace xunfei abnf, ** sort factor(two way: main, jiandu),
*** TODO [#C] es search note etc.
*** mongoDB connector    data fact  mapping
- oplog, error happened
- connector from mongoDB to es implement, 
- mapping
*** mongo-connector
**** from big function completion to small detail
- installation
  - pip install mongo-connector
- get started
  - [ ] install mongo-connector
  - [ ] start up a mongo replica ::
    - monogod --replSet setName
    - mongo
    - rs.initiate()
  - [ ] start up elasticsearch
  - [ ] start up mongo-connector with -m mongodb_source -t target_system  -d doc_manager
- ? :: configuration file : config.json   dot-notation
  - "__xxxx" will be ignored, can be used to comment
**** system overview 
1. main thread creates oplogthread based on mongos? mongod? information,
   - initialize docmanger for each replication endpoint,
   - provied the docmangers to oplogthread
2. oplogthread create an tailable cursor into oplog.rs collection of mongod
3. oplogthread initiate a "*collection dump*",by which it upsert document throuth docmanager
   - happen only first time mongo-connector is started
   - do not happen again as long as mongo-connector can find the last timestamp
4. oplogthread poll oplog.rs for new documents corresponding one operation
   - insert :: docmanger upsert method
   - update :: retrieve version of document on the remote system,apply
     - apply update, and resave it on the remote system:: 
   - delete :: remove document with id from remote system
   - other database command :: docmanger handle_command method
5. oplogthread notes the timestamp from oplog.rs and write out to oplog.timestamp
   - only happen untill mongo-connector is killed
   - other cases will cause oplogthread to take some actions
**** oplog.timestamp
- collection dumps ::
  - when oplog progress file is not found, mongo-connector will begin data form
    - all mongoDB collections in colllection dumps
  - then the oplog.timestamp will be updated with most recent timestamp from
    - before the dumps happened
  - then apllies all operations form before the dump happened
  - can force the dump to happen
- format :: ["oplog name", timestamp]
- creation ::
  - created as the final step of mongo-connector initialization
    - or happens with or without a collection dump
- update ::
  - main thread monitor the progress of oplog-tailing thread
    - main threads of connector update the progress file per second
  - oplog-tailing thread publish their progress at following times:
    -  
**** oplog progress file : 
- created with collection dump which initiated by opload thread which created by main thread
- updated usually by main thread, oploathread write out to if if main thread were killed
* <2015-10-10 周六 10:11> [1/6] 
** TODO familiar to mongo-connector to make it instinct [0/2]
- [ ] systemd manage monogo-connector
- [ ] learn systemd
** TODO [#C] convert abnf to regex
- efficeny
- dict in regex
** DONE [#A] finish write elasticsearch java  
CLOSED: [2015-10-10 周六 12:56]
- partial_fields :: only display data loaded from _source
- fields :: will display all the segments
** TODO [#A] elasticsearch mapping fields images mapping
- answer and Qa docmanger in Mongo-connector
** TODO [#B] elasticsearch aggregation, modeling data, handle relationship
- qa mapping
- mongo connector self docmanager
** index meaning
- locality :: city and big view area
- viewspot ::
- ::
* <2015-10-12 周一 10:53>[2/3] 
** DONE finish poi elasticsearch java
CLOSED: [2015-10-12 周一 13:55]
- for the moment : set one kind of sentence
** TODO test script score query
** DONE go back to school
CLOSED: [2015-10-13 周二 10:07]
* <2015-10-13 周二 10:07>[/] 
** write a small mongo-connector demo
*** target :: implement functions: only synchronize pointed index[2/2]
  - enviroment :: open mongo, open es, open mongo-connector
  - [X] test self-name doc_manager in mongo-connector :: rename elastic_doc_manager to self-name, 
  - [X] change my_doc_manager :: firstly operation pointed index
*** qa mongo-connector [0/3]
- 'mongodb://tar:Iv7seV8niv0m@119.254.100.93:32001/andaman'
  - Question
  - Answer
- attention : the logic and operation
- [ ] make clear qa and es:qa
- [ ] basic operation
  - insert :: upsert
    - divide QA into question and answer
      - question 
      - answer 
  - update :: update
  - del :: remove
- [ ] maping correspond
* <2015-10-14 周三 10:50> 
** mongo-connector 
- mongo-connector -m localhost:27017 -t http://192.168.200.3:9210 -d qa_doc_manager
- mongoDB repl
  - mongod --replSet singleNodeRepl  // later : mongoDB replicate set
  - rs.initiate()
  - mongod --dbpath
** DONE first review own code : like a compiler
CLOSED: [2015-10-14 周三 17:21]
** run it testly
** haizi education:
- pure function ::
  - 确定性 :: yes 
  - 副作用 :: no
  - do not read global variable :: 
- python start with '_' :: private 
  - case : _index private method of one class
  - can get forcely : getattr('class_name', 'privat_method')
- 幂等 :: the result does no change after execute same operation many times
- Python :: 
  - write decorator
  - global various
* <2015-10-15 周四 11:14> [/] 
** test qa_doc_manager 
- right cmd :: mongo-connector -m ... -t ... -d ...
- if oplog had existed, operation on mongoDB will trigger the test doc manager
  - del oplog:: 
  - test operation :: write code??
    - remove ::
    - insert ::
    - update ::
** mongo-connector
- es error : not enough space caused?
- mongo-connector will exist if encounter es error
- error -> find log file to search reason
- alias mechanism in es to avoid change code in Python
** deal with question 
- first solve problems
- consult the issues in the process of solving problems
** Python
- "is" and "==" usage
- if not bool_True:
      return 
  else :
- used to minus indentation
** decorator
- decorator is in common use 普适性
- *args :: array
- **kwargs :: keyword = value  {key : value}
- multi decorator :: 
** make function
** pep8 : Python name regulation
- camel : Java, mongoDB
- snake : Python, ElasticSearch
** catch cat neck
- 1. safe
- 2. save power
- 3. make cat quite :: gene
** Python and C
ctypes
* <2015-10-16 周五 11:05> 
- pycharm key :: C-f8 :
** Python
- staticmethod :: in one class, if its method did not use "self", should be static
** test cases:
- mongo-connector
* <2015-10-19 周一 11:05> 
** TODO note the initial interest 
** the two roads : js, python, mobile end
